= Autorisations requises
:revdate: 2025-07-10
:page-revdate: {revdate}
:description: SUSE Observability en mode auto-hébergé

== Vue d'ensemble

Tous les composants de SUSE Observability peuvent être exécutés sans aucune autorisation supplémentaire. Cependant, pour installer SUSE Observability avec succès, vous devez disposer de certains privilèges supplémentaires ou vous assurer que les conditions décrites dans cette page sont remplies.

== Authentification et autorisation entre services

Pour permettre la communication entre les services SUSE Observability, SUSE Observability utilise des comptes de service Kubernetes. Pour pouvoir vérifier leur validité et leur rôle, le tableau de bord crée une ressource `ClusterRole` et une ressource `ClusterRoleBinding`. La création de ces ressources à l'échelle du cluster est souvent interdite aux utilisateurs qui ne sont pas administrateurs de Kubernetes/OpenShift. Dans ce cas, la création peut être désactivée et les rôles et les liaisons de rôles doivent être xref:/setup/install-stackstate/kubernetes_openshift/required_permissions.adoc#_manually_create_cluster_wide_resources[créés manuellement] par votre administrateur de cluster.

=== Désactiver la création automatique de ressources à l'échelle du cluster

La création automatique de ressources à l'échelle du cluster lors de l'installation de SUSE Observability peut être désactivée en ajoutant la section suivante au fichier `values.yaml` utilisé pour installer SUSE Observability :

[tabs]
====
values.yaml::
+
--

[,yaml]
----
cluster-role:
  enabled: false
----

--
====

[NOTE]
====
Si la création du rôle de cluster et de la liaison de rôle de cluster a été désactivée, veillez à suivre les instructions xref:/setup/install-stackstate/kubernetes_openshift/required_permissions.adoc#_manually_create_cluster_wide_resources[ci-dessous] pour les créer manuellement à l'aide des xref:/setup/install-stackstate/kubernetes_openshift/required_permissions.adoc#_manually_create_cluster_wide_resources[instructions ci-dessous.]
====


=== Créer manuellement des ressources à l'échelle du cluster

Si vous devez créer manuellement les ressources à l'échelle du cluster, demandez à votre administrateur Kubernetes/OpenShift de créer les 3 ressources ci-dessous dans le clsuter.

[NOTE]
====
Vérifiez que vous spécifiez le compte de service et l'espace de noms corrects pour la ressource `ServiceAccount` liée aux deux ressources `ClusterRoleBinding`. L'exemple suppose que l'espace de noms `suse-observability` est utilisé et que `suse-observability` est utilisé comme version. Si un autre espace de noms est utilisé, il faut modifier l'espace de noms dans les exemples. Les comptes de service référencés doivent également être modifiés en `<release>-suse-observability-api`.
====


[tabs]
====
clusterrole-authorization.yaml::
+
--

[,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: suse-observability-authorization
rules:
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - rolebindings
  verbs:
  - list
----

--
====

[tabs]
====
clusterrolebinding-authentication.yaml::
+
--

[,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: suse-observability-authentication
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: suse-observability-api
  namespace: suse-observability
----

--
====

[tabs]
====
clusterrolebinding-authorization.yaml::
+
--

[,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: suse-observability-authorization
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: suse-observability-authorization
subjects:
- kind: ServiceAccount
  name: suse-observability-api
  namespace: suse-observability
----

--
====

== Normes de sécurité pour les pods

Si votre cluster Kubernetes a https://kubernetes.io/docs/concepts/security/pod-security-standards/[activé], vous devez configurer des politiques de sécurité appropriées pour l'espace de noms `suse-observability`. SUSE Observability nécessite la norme de base Pod Security Standard pour fonctionner correctement.

=== Configuration des normes de sécurité des pods

Appliquer la norme de sécurité Pod de base à l'espace de noms `suse-observability`:

[,bash]
----
kubectl label ns suse-observability pod-security.kubernetes.io/enforce=baseline --overwrite
kubectl label ns suse-observability pod-security.kubernetes.io/audit=baseline --overwrite
kubectl label ns suse-observability pod-security.kubernetes.io/warn=baseline --overwrite
----

[NOTE]
====
Si les normes de sécurité Pod sont activées dans votre cluster, vous devez vous assurer que les conditions préalables d'Elasticsearch sont correctement configurées avant de déployer SUSE Observability.
Étant donné que la norme de base Pod Security Standard n'autorise pas les conteneurs privilégiés, vous devez suivre les xref:/setup/install-stackstate/kubernetes_openshift/required_permissions.adoc#_elasticsearch[instructions] pour configurer le paramètre de noyau `vm.max_map_count` requis au niveau de l'hôte.
====


== Elasticsearch

SUSE Observability utilise Elasticsearch pour stocker ses index. Les nœuds sur lesquels Elasticsearch s'exécute doivent répondre à certaines exigences supplémentaires.

Comme la configuration du système Linux `vm.max_map_count` est généralement inférieure à celle requise pour le démarrage d'Elasticsearch, un conteneur init est utilisé en mode privilégié et en tant qu'utilisateur racine. Le conteneur init est activé par défaut pour permettre de modifier les paramètres du système `vm.max_map_count`.

=== Désactiver le conteneur Elasticsearch init privilégié

Si vous ou vos administrateurs Kubernetes/OpenShift ne souhaitez pas que le conteneur init privilégié Elasticsearch soit activé par défaut, vous pouvez désactiver ce comportement dans le fichier `values.yaml` utilisé pour installer SUSE Observability :

[tabs]
====
values.yaml::
+
--

[,yaml]
----
elasticsearch:
  sysctlInitContainer:
    enabled: false
----

--
====

[NOTE]
====
Si cette fonction est désactivée, vous devrez vous assurer que le paramètre `vm.max_map_count` est modifié de sa valeur par défaut habituelle de `65530` à `262144`. Si cela n'est pas fait, Elasticsearch ne démarrera pas et ses pods seront dans une boucle de redémarrage.
====


Pour vérifier le paramètre actuel de `vm.max_map_count`, exécutez la commande suivante. Notez qu'il s'agit d'un pod privilégié :

[,text]
----
kubectl run -i --tty sysctl-check-max-map-count --privileged=true  --image=busybox --restart=Never --rm=true -- sysctl vm.max_map_count
----

Si le paramètre actuel de `vm.max_map_count` n'est pas au moins `262144`, il devra être augmenté d'une autre manière ou Elasticsearch ne démarrera pas et ses pods seront dans une boucle de redémarrage. Les journaux contiendront un message d'erreur comme celui-ci :

[,text]
----
bootstrap checks failed
max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
----

=== Augmenter les paramètres du système Linux pour Elasticsearch

Selon les préférences de vos administrateurs Kubernetes/OpenShift, le site `vm.max_map_count` peut être défini à une valeur par défaut plus élevée sur tous les nœuds, soit en modifiant la configuration par défaut du nœud (par exemple via des scripts init), soit en demandant à un DaemonSet de le faire juste après le démarrage du nœud. La première dépend fortement de la configuration de votre ordinateur, il n'y a donc pas de solution générale.

Voici un exemple qui peut servir de point de départ à un DaemonSet pour modifier le paramètre `vm.max_map_count`:

[,yaml]
----
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: set-vm-max-map-count
  namespace: kube-system
  labels:
    k8s-app: set-vm-max-map-count
spec:
  selector:
    matchLabels:
      name: set-vm-max-map-count
  template:
    metadata:
      labels:
        name: set-vm-max-map-count
    spec:
      # Make sure the setting always gets changed as soon as possible:
      tolerations:
      - effect: NoSchedule
        operator: Exists
      - effect: NoExecute
        key: node.kubernetes.io/not-ready
        operator: Exists
      # Optional node selector (assumes nodes for Elasticsearch are labeled `elasticsearch:yes`
      # nodeSelector:
      #  elasticsearch: yes
      initContainers:
        - name: set-vm-max-map-count
          image: busybox
          securityContext:
            runAsUser: 0
            privileged: true
          command: ["sysctl", "-w", "vm.max_map_count=262144"]
          resources:
            limits:
              cpu: 100m
              memory: 100Mi
            requests:
              cpu: 100m
              memory: 100Mi
      # A pause container is needed to prevent a restart loop of the pods in the daemonset
      # See also this Kubernetes issue https://github.com/kubernetes/kubernetes/issues/36601
      containers:
        - name: pause
          image: busybox
          command: ["sleep", "infinity"]
          resources:
            limits:
              cpu: 50m
              memory: 50Mi
            requests:
              cpu: 50m
              memory: 50Mi
----

Pour limiter le nombre de nœuds auxquels cette mesure s'applique, les nœuds peuvent être étiquetés. Les NodeSelectors de ce DaemonSet, comme indiqué dans l'exemple, et le déploiement d'Elasticsearch peuvent alors être configurés pour s'exécuter uniquement sur les nœuds portant l'étiquette spécifique. Pour Elasticsearch, le sélecteur de nœud peut être spécifié via les valeurs :

[,yaml]
----
elasticsearch:
  nodeSelector:
    elasticsearch: yes
  sysctlInitContainer:
    enabled: false
----

== Voir aussi

* xref:/setup/install-stackstate/kubernetes_openshift/kubernetes_install.adoc[Installer SUSE Observability sur Kubernetes]
* xref:/setup/install-stackstate/kubernetes_openshift/openshift_install.adoc[Installer SUSE Observability sur OpenShift]
